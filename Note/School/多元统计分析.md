# 第二章 多元线性模型

## 多元线性模型

### 模型定义

多元线性模型通常用来描述变量y与多个x变量之间的随机线性关系，即y = β0 + β1x1 + ... + βpxp + ε  
x1, x2, ..., xp是非随机的自变量，y是随机的因变量，β0是常数项，β1, β2, ..., βp是回归系数，ε是随机误差项

若对y和x进行n次观测，得到n组观测值，引入矩阵，则上式可写成y = Xβ + ε  
式中，y是n×1观测向量，X是n×(p+1)已知设计矩阵，β是(p+1)×1未知参数向量，ε是n×1随机误差向量

若y = Xβ + ε模型为普通线性回归模型，且模型的随机误差项服从正态分布，则称模型为普通正态线性回归模型

### 模型的参数估计和检验

R中的回归分析过程为 ***lm()***，可以完成回归系数的估计，以及方程和回归系数的显著性检验

**lm()** 的作用是建立线性回归方程，使用方法如下  
lm.exam <- lm(y~x1+x2+x3+x4+x5, data = d2.1)

## 变量选择

最优回归方程指的是从可供选择的所有自变量中选出对因变量有显著影响的变量来建立回归方程

R中的逐步回归法的计算函数 ***step()*** 是以AIC为模型选择准则来选择变量，AIC值越小，认为模型越好

**step()** 的作用是进行逐步回归，使用方法如下  
lm.step <- step(lm.exam, direction = "both")

## 回归诊断

### 残差分析和异常点探测

残差分析可以诊断模型的基本假设是否成立

在R中，分别采用 ***residuals(), rstandard()和rstudent()*** 来计算普通残差、标准化残差和学生化残差

如果回归模型能够很好的描述拟合的数据，那么残差对预测值的散点图应该像一些随机散布的点，如果某个点的残差很大，则说明这个点偏离数据主体比较远，**一般把标准化残差的绝对值大于等于2的观测值认为是可疑点，大于等于3的观测值认为是异常点**

若绘制散点图后发现残差的分布有随预测值增大而减小的趋势，则同方差性的基本假定可能不成立，此时可通过对因变量作适当方差稳定变化来解决方差非齐问题，常见的方差稳定变换有：
1. 对数变换
2. 开方变换
3. 倒数变换
4. Box-Cox变换

例如对模型进行对数变换  
lm.step_new <- update(lm.step, log(.)~.)

找出异常的观测值后，去掉这些观测值再建立全变量回归方程，例如  
lm.exam <- lm(log(y)~x1+x2+x3+x4, data = d2.1[-c(19),])

### 回归诊断的一般方法

R中使用函数 ***plot()和influence.measures()*** 绘制诊断图和计算诊断统计量，使用方法如下  
plot(lm.step_new)  
influence.measures(lm.step_new)

## 回归预测

回归预测分为点预测和区间预测两种，在R中，***predict()*** 用来计算回归模型的预测值，例如  
preds <- data.frame(x2 = 80, x3 = 90)  
predicts(lm.step, newdata = preds, interval = "prediction", level = 0.95)  
其中interval = "prediction"表示要给出区间预测

# 广义线性模型

## Logistic模型

y服从二项分布

R中的广义线性模型过程 ***glm()*** 可以完成回归系数的估计，以及模型回归系数的显著性检验，例如  
glm.logit <- glm(y~x1+x2, family = binomial(link = logit), data = d3.1)

其中link = logit即为使用Logistics模型

## Probit模型

y服从二项分布

即将**glm()函数中的family = binomial(link = ...) 写为link = probit**

## 多项Logit模型

Logistic和Probit模型的因变量为二水平分类变量，分类变量有两个以上的水平且这些水平为仅有的可能水平时，可才用多项Logit模型

采用 ***nnet包中的multinom()*** 函数可以完成多项Logit模型的拟合，使用方法如下  
mlog <- multinom(y~x1+x2, data = d3.2)

## 泊松对数线性模型

y服从泊松分布

glm.ln <- glm(y~x1+x2+x3, family = poisson(link = log), data = d3.3)

## 零膨胀计数模型

y服从泊松分布，但y取0的可能性很大

需要实现加载程序包pscl，例如  
library(pscl)  
a2 <- zeroinfl(deaths~hiv+fac+age+py, data = d3.4)

## 多项分布对数线性模型

w.fit <- glm(Freq~Class+Sex+Age, family = poisson, data = w, offset = log(n))

# 聚类分析

对样品的聚类称为**Q型聚类**，对变量的聚类称为**R型聚类**

将类由多变少的称为分层聚类法，也称为系统聚类法，将类由少变多的称为分解聚类法

## 相似性度量

- Q型聚类根据样品之间的靠近程度来进行聚类，样品间的靠近程度通常用距离来衡量

    度量点与点之间靠近程度的距离有六种，最常用的是欧氏距离(直接距离)和马氏距离(考虑了变量之间的相关性，消除了变量单位不一致的影响)

    在R中可使用dist()函数来计算距离

- R型聚类根据变量之间的相似程度来进行聚类，常用变量间的相似系数来度量，相似系数绝对值越接近1，两个变量之间的关系越密切

    通常使用两种相似系数：相关系数和夹角余弦

    在R中可使用scale()函数对数据进行中心化或标准化，进而计算夹角余弦；可使用cor()函数来计算相关系数

## 系统聚类法

系统聚类的基本步骤是先将每个个体各自看成一类，设共有r类，根据个体之间的相似程度将r类个体间最相似的两类合并得到r-1类，重复该步骤直到r个个体合成一个大类，最后作出树形图，按一定的原则决定分成几类

对于Q型聚类，根据最小、最大、中间等距离定义衡量距离的方法，对于R型聚类，通常只使用相似系数的最大值作为衡量方法，但也可以将相似系数转换为变量间的距离，例如d = 1 - c和d^2 = 1 - c^2，然后仿照Q型聚类的方法

R中的函数为hclust()

## K均值聚类法

系统聚类法每一步都需要计算类间距离，计算量大，K均值聚类法的基本思想是先根据给定的参数k，把n个对象粗略的分为k类，然后按照某种最优原则(通常表示为一个准则函数)修改不合理的分类，直到准则函数收敛为止

R中的函数为kmeans()

## EM聚类法

算法认为全体观测值可被分为k个自然小类，其中每个小类所包含的观测值来自一个特定的统计分布总体，全体观测值是来自k个统计分布总体的混合样本。EM聚类的难点在于不但分类数k是未知的，各个观测值归入各个总体的概率也需要逐个计算

R中的函数为Mclust()

# 判别分析

判别分析是在已知样品分类的前提下将给定的新样品按照某种分类准则判入某个类中，判别规则通常根据已有的数据或者现有的部分样品数据作为样本建立起来的

聚类分析和判别分析的主要区别在于，聚类分析事先不知道应该分成几类，作判别分析时已经明确分成几类，主要工作是利用样本建立判别准则

## 距离判别

在判别分析中，马氏距离比欧氏距离更常用，因为马氏距离考虑了样品数据之间的依存关系，消除了变量单位不一致的影响，更具合理性

由于两个马氏距离互相比较时，马氏距离的定义式与马氏距离的平方等价，所以考虑两个马氏距离平方的差，W为样品X到总体G1的马氏距离和到总体G2的马氏距离的平方差，若W≥0则X属于G1，否则属于G2，这样通常称W为判别函数，它是X的线性函数

两个总体的均值分别为μ1和μ2，μ1和μ2需要有显著差异才能作距离判别

## Fisher判别

有线性判别、非线性判别和典型判别等方法

Fisher判别主要是将多维数据投影到一维直线上，使得同一类别中的数据在直线上尽量靠拢，不同类别尽量分开，也就是组内方差尽量小，组间方差尽量大，然后再利用距离判别法来建立判别准则

y = a^Tx，当a = c∑^-1(μ1 - μ2)，c = 1时，线性函数y称为Fisher线性判别函数

记μy = a^Tμ，μ为1/2(μ1 + μ2)，当y≥μy时x属于G1，否则属于G2

在R中使用lda()函数进行线性判别

## Bayes判别

Bayes判别假定对研究对象已经有一定的认识，这种认识可以用先验概率来描述，当取得样本时，可以利用样本来修正已有的先验概率，得到后验概率，再通过后验概率进行统计推断

Bayes判别属于概率判别法，判别准则时以个体归属某类的概率最大或错判总平均损失最小为标准

## 二次判别

二次判别属于距离判别法，以两总体的距离判别法为例，当总体G1和G2各自的协方差矩阵不相等时，判别函数因为表达式不可化简而不再是线性函数而是二次函数了，使用二次判别函数进行判别的方法叫做二次判别法。当不同总体的协方差矩阵不同时，应该使用二次判别法

在R中使用qda()函数进行二次判别

# 主成分分析

既要减少变量个数，又不能损失太多信息

## 总体主成分

### 主成分的含义

主成分是对原始变量的信息进行综合而得的变量，形式上表现为各个原始变量的线性组合，通过对相关系数矩阵进行特征分解来找到主成分

p个变量可以生成p个主成分，按照方差从大到小依次为第一主成分到第p主成分，最后选择使用的k个主成分要满足：
- 保留原始变量的大部分信息
- k << p
- 各个主成分之间互不相关

其中各个主成分的方差之和等于原始变量的方差之和

主成分和原始变量满足：y = P^TX，可变形为X = Py

正交阵P为原始变量X关于主成分y的载荷矩阵

### 主成分个数的确定

设wi为主成分yi的方差贡献率，y1的方差贡献率最大，依次递减。∑wi为前k个主成分的累计贡献率，通常取累计贡献率>80%的最小的k为主成分的个数

## 主成分分析的步骤

在实际应用中通常使用样本的相关系数矩阵R来进行主成分分析，具体步骤可归纳为：
1. 将原始样本数据标准化
2. 求样本的相关系数矩阵R
3. 球R的p个特征值以及相应的单位正交特征矩阵
4. 按主成分累计贡献率超过80%确定主成分的个数k，并写出样本主成分的表达式
5. 对分析结果做出统计意义和实际意义两方面的解释

常用的函数有：
1. princomp()
2. summary()，用于提取主成分的信息
3. loadings()，用于显示主成分分析中载荷阵的内容
4. predict()，用于预测主成分的值
5. screeplot()，用于画出主成分的碎石图

# 因子分析

因子分析就是用少数几个潜在的不能观察的随机变量(称为因子)取描述多个随机变量之间的协方差关系。基本思想是个悲剧相关性大小把变量分组，使组内的变量相关性较高，但不同组的变量相关性较低，则每组变量可以代表一个基本结构，称为因子。研究变量之间的相关关系称为R型因子分析，研究样品之间的相关关系称为Q型因子分析

## 正交因子模型

假定p维随机变量X的期望为μ，协方差矩阵为∑，线性地依赖于少数几个不可观测的随机变量F和p个附加的方差ε。随机变量F称为公因子，ε称为误差或者特殊因子

则因子模型可以写成：X = μ + AF + ε

其中aij称为第i个变量在第j个因子上的载荷，矩阵A称为载荷矩阵

由Var(xi) = ai1^2 + ai2^2 + ... + aim^2 + Φi得：xi的方差由两部分构成，m个公因子和一个特殊因子

其中aij^2表示第j个公因子对xi的方差贡献，Φi表示第i个特殊因子对xi的方差贡献，称为特殊度，hi^2 = ai1^2 + ... + aim^2表示m个公因子对变量xi的方差贡献总和，称为第i个共同度，它是载荷矩阵A的第i行元素的平方和

考虑某个公因子对各个变量的影响，可以用bj^2 = a1j^2 + ... + apj^2来度量，称为公因子fj对p个变量的方差贡献

## 因子模型的估计

要建立因子模型，首先要估计载荷矩阵A及特殊方差Φi

### 主成分法

要寻找m个公因子，如果协方差矩阵∑最后的p - m个特征值很小，可以去掉这些公因子，假定特殊因子可以在∑的分解中忽略

实际应用中协方差矩阵∑是未知的，通常用它的估计，即样本协方差矩阵S来代替，考虑到变量的量纲差别，需要进行数据标准化，这样求得的样本协方差矩阵就是原来数据的相关系数矩阵R

可以使用主成分分析法的思想，以公因子对各个变量的累计贡献率∑bj^2≥80%作为标准来确定公因子数m

### 主因子法

### 极大似然法

主成分法和主因子法是在求解过程中确定因子数m的，而极大似然法必须在求解之前确定m

## 因子正交旋转

满足方差结构∑ = AA^T + Φ的因子模型不唯一，模型的公因子与载荷矩阵也不唯一。同时，公因子和因子载荷矩阵作正交变换后不改变共同度，称因子载荷的正交变换和伴随的因子正交变换为因子正交旋转

A* = AT

因子旋转的目的是，如果初始载荷不易解释，就需要对载荷作旋转，得到一个更简单的结构。最理想的情况是：每个变量仅在一个因子上有较大的载荷，在其余因子上的载荷比较小

因子旋转不改变共性方差和残差矩阵，旋转后的因子往往更有实际意义

## 因子得分

因子得分，即公因子的估计值，因子得分的计算不同于参数估计，而是对不可观测的因子fi取值的估计

### 加权最小二乘法

无偏的

### 回归法

有偏的

### 综合因子得分

以各因子的方差贡献率为权重，由各因子的线性组合得到的综合评价指标函数

f = ∑wifi

其中wi = λi / (λ1 + λ2 + ... + λm)