# 从数据处理开始

## 数据理解

- 机器学习的任务是：回归、分类和无监督学习
- 监督学习是使用已知的训练样本，而非监督学习直接使用数据进行建模分析
- 数据挖掘的六大任务是：分类、聚类、回归、关联、序列和异常检测

## 单变量数据探索

每次只分析1个变量,单变量数据探索主要目的是对单个变量的数据分布进行查看，方便后续的数据预处理

常见的单变量分析方法有：
- 频数分析：分析类别型变量每个值出现的频数(**用value_count()方法进行频数统计**)，还可以通过饼图或柱状图可视化展现
- 描述性统计：对数值型变量的常用统计量进行分析(**用describe()方法，quantile()方法指定分位数统计**)
- 直方图：对数值型变量进行分段处理后进行频数分析并作图(**用hist()方法作直方图**)

## 多变量数据探索

不是一次性探索多个单变量，而是同时探索多个变量之间的相关关系

两个变量之间的相关性分析方法：
- 两个数值型变量：散点图
- 两个类别型变量：交叉表、堆栈条图
- 一个数值型一个类别型变量：分类汇总统计

## 数据预处理

主要目的是为机器学习建模进行数据准备，常做的数据预处理工作有：
- 对缺失值的处理
- 对类别型变量的值进行重新编码
- 把连续性变量进行分箱，然后再按照处理类别型变量的方式重新编码
- 对连续性变量进行标准化和归一化处理

### 缺失值处理

1. 直接把有缺失值的整条记录删除，适合数据样本较大而缺失记录较少的场景，删除缺失记录对整体影响很小
2. 构造一个新变量来标记缺失值：缺失标记为1，不缺失标记为0，适合缺失值本身是一个有意义的信息
3. 用一个值替换掉缺失值

### 类别变量重新编码

机器学习算法要求输入的变量值必须是数值

针对类别型变量必须重新编码，转换为数值型变量，信息量还不会丢失。最常用的一种类别变量编码方式是哑变量编码，也叫独热编码

独热编码就是把1个类别型变量转化为N个0/1标识变量，类别型变量有多少个类别值，转化后就有多少个0/1标识变量

### 连续变量分箱

分箱可以让连续型变量变成类别型变量，直接做频数分析或者直方图，便于分析

变量分箱让模型变得更稳健，不太容易过拟合

- 自定义分箱
- 等宽分箱：每个箱的边界呈等差数列
- 等深分箱：保证每个箱内记录数一样或满足指定的比例

### 标准化和归一化

标准化和归一化都属于对变量进行无量纲化处理

无量纲化处理的作用：
- 让不属于同一量纲的特征值可以比较
- 无量纲化后模型收敛会加快，即运行速度变快
- 对一些模型的结果影响较大，一般是涉及到距离计算的模型和算法

## 特征变量的构造和组合

又称为特征工程

- 多个变量之间的数学运算
- 基于逻辑判断来构造衍生变量
- 多个特征变量进行组合

# 特征工程之数据预处理

## 非数值类型数据处理

### 哑变量处理

哑变量也叫虚拟变量，构造取值为0或1的变量，将类型变量换成数字1和0，使用get_dummies()函数进行哑变量处理

因为存在多重共线性(即通过n-1个变量可判断1个变量的值为0还是1)，所以要删去一个哑变量

### 编号处理

## 重复值、缺失值及异常值处理

### 重复值处理

使用duplicated()函数来查询重复的内容

使用sum()函数统计重复行的数量

使用drop_duplicates()函数删除重复行

### 缺失值处理

使用isnull()函数或isna()函数来查看空值，是空值则为True，否则为False

使用dropna()函数可以删除空值，设置thresh参数为n，则删除非空值少于n个的行

使用fillna()可以填补空值，method = 'pad'代表用缺失值所在列的前一个值填充，若前一个值也缺失，则结果不变，method = 'backfill'使用后一个值，limit参数可以限制每列能替换的缺失值个数

### 异常值处理

主要通过箱型图观察和标准差检测异常值

使用boxplot()函数可以绘制箱型图

异常值常见处理方式有：
1. 删除含有异常值的记录
2. 视为缺失值，使用缺失值处理的方法进行处理
3. 使用数据分析来进行异常值处理
4. 如果异常值较少或影响不大也可以不处理

## 数据分箱

使用cut()函数进行分箱

## 特征筛选：WOE值与IV值

### WOE值

WOE的全称是证据权重，要计算一个变量的WOE值，需要首先将变量进行分箱处理，分箱后，对于第i组分箱内的数据，该分箱的WOE值使用WOE值的计算公式计算：  
- WOEi = ln(pyi/pni)
- pyi = yi/yT    pni = ni/nT

pyi是第i组分箱中目标变量标签取值为1的个体占整个数据中1的比例  
pni是第i组分箱中目标变量标签取值为0的个体占整个数据中0的比例  
yi是第i组分箱中1的个体数量  
yT是整个样本中所有1的个体数量
ni是第i组分箱中0的个体数量  
nT是整个样本中所有0的个体数量

### IV值

IV值能较好的反应特征变量的预测能力，特征变量对于预测结果的贡献越大，IV值越大，IV值的计算公式：  
- IVi = (pyi - pni) × WOEi
- IV = ∑IVi

### WOE值与IV值

使用IV值而不直接使用WOE值的原因

1. 人们习惯用一个大于0的数值取衡量预测能力，WOE是有可能取到负值的
2. 通过pyi - pni作为权重因子可以体现分组中数据量占整体的比例

IV值越高，说明该特征变量越具有区分度，但是IV值不是越大越好，当IV值大于0.5时，有时需要对这个特征打疑问，因为有些过好。**通常**会选择IV值在0.1~0.5这个范围的特征

## 多重共线性的分析与处理

### 多重共线性的定义

对于多元线性回归模型而言，如果特征变量间存在高度线性相关关系则成为多重共线性

多重共线性有完全共线性和近似共线性
- 完全共线性：X1 = -(X2 + X3 + ... + Xn)
- 近似共线性：a1X1 + a2X2 + ... + anXn + v = 0

多重共线性在实际应用中带来的不利影响：
1. 线性回归估计式变得不确定或不精确
2. 线性回归估计式方差变得很大，标准误差增大
3. 当多重共线性严重时，可能使估计的回归系数符号相反，得出错误的结论
4. 削减特征重要性程度

### 多重共线性分析与检验

#### 相关系数判断法

#### 方差膨胀因子法(VIF检验)

方差膨胀因子的计算公式是：VIFi = 1 / (1 - Ri ^ 2)

其中R^2是自变量的方差膨胀系数

VIF越大，表示自变量与其他自变量的共线性越强，一般VIF < 5表示没有共线性，5 < VIF < 10表示弱复共线性，10 < VIF < 100表示中等共线性，VIF > 100表示严重共线性

## 过采样和欠采样

### 过采样

重复正比例数据，实际上没有为模型引入更多数据，过分强调正比例数据，会放大正比例噪音对模型的影响，存在过拟合问题

#### 随机过采样

从样本中随机抽取旧样本作为一个新样本

#### SMOTE法过采样

合成少数类过采样技术，改进了随机过采样容易模型过拟合的问题

随机选取少数类中的一个样本点，找到离该样本点最近的4个样本点，在选中的样本点和这4个样本点分别连成的4条线段上随机选取4点生成新的样本点，对少数类中的其余所有样本点重复以上步骤直到样本个数达到目标为止

### 欠采样

与过采样类似，但欠采样的目的是将数字与最低样本量匹配


# 数据降维之PCA主成分分析

## 数据降维

数据降维的方法主要有两种：选择特征和抽取特征

选择特征即从原有的特征中挑选出最佳的特征，抽取特征即将数据由高维向低维投影，进行坐标的线性转换

PCA主成分分析即为典型的抽取特征的方法，它不仅是对高维数据降维，更重要的是经过降维去除噪声，发现数据中的模式

### 二维空间降维

可以将y = x这条直线作为新的坐标轴x'，在实际进行数据降维前首先需要对特征数据零均值化，即对每个特征维度的数据减去该特征的均值

所以对于二维到一维的数据降维，本质就是在将原始数据零均值化后，寻找合适的x' = αX + βY的线性组合系数α和β

### n维空间降维

调用计算库

其中需要满足的线性代数条件：
1. 每个主成分的系数平方和为1
2. 各个主成分互不相关
3. 主成分的方差依次递减，重要性依次递减

# 线性判别分析

常用的降维技术有PCA和LDA
- 无监督方法，主要是主成分分析法，简称PCA
- 有监督方法，主要是线性判别分析法，简称LDA

线性判别分析是一种监督学习的将为技术，投影后希望**类内方差最小，类间方差最大**，即每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大

## PCA和LDA的区别

PCA降维并没有指定目标变量y，它的目标是寻找能让方差最大化的投射轴(主成分)，所以PCA降维的优化目标是尽量让降维后的信息损失最少

LDA降维的目标是能在低维空间中最大化的把目标变量(类别型变量)的每一个类别值区分开，LDA本质上是一种分类算法

## PCA和LDA的比较

### 相同点

1. 两者均可以对数据进行降维
2. 两者在降维时均使用了矩阵特征分解的思想
3. 两者都假设数据符合高斯分布

### 不同点

1. LDA是有监督的降维方法，而PCA是无监督的降维方法
2. LDA降维最多降到类别数k-1的维数，而PCA没有这个限制
3. LDA除了可以用于降维，还可以用于分类

# 高级特征工程

## RFM特征提取

RFM是CRM(客户关系管理)领域中广泛使用的一个评估用户价值的指标，CRM客户数据库中有3个要素，构成了CRM数据分析最好的指标：
- 最近一次购买时间(Recency)
- 购买频率(Frequency)
- 购买金额(Monetary)

RFM不止适用于CRM领域，这种交易数据十分常见，所以RFM就是用来衡量用户行为特征最重要的一种方法

### RFM特征离散化

原始提取出来的RFM特征都是连续型的数值，RFM分析的时候我们习惯进行离散化(分箱)，常见的离散化方法是等深分段的方法

## 时间序列

### 时间序列数据的类型

主要有两种：经典时间序列、带时间戳的事件流

#### 经典时间序列

包含随时间测量的一系列数值，这些时间点分布是均匀的，例如每小时，每天，每周，每月等。典型的例子有：股票每天的收盘价，每天的最高气温和最低气温

#### 带时间戳的事件流

包含离散的一系列事件的时间戳，以及对应的事件相关的信息。典型的例子有：用户访问网站的点击事件流，记录了用户每次点击的时间以及网页URL；用户银行账户的交易记录，包含了每次交易的时间、类型以及金额

带时间戳的事件流如果是关于用户的，这种类型的数据就是RFM所处理的交易型数据

### 时间序列预测方法

1. 时间序列预测：基于过去的序列值对未来的序列值进行预测，例如：
    - 预测明天某只股票的价格
    - 预测明天的天气

2. 时间序列分类/回归：对成百上千的时间序列进行分类或回归，而不是预测单个时间序列的数值

单个时间序列的预测问题可以转换为时间序列分类/回归问题，例如：对一只股票未来几天的价格进行预测，可以抽取这只股票过去每天对应的序列形成数据集，假设有过去一千天的数据，转换后可以得到1000个样本的数据集，就可以把问题转换为时间序列分类/回归

## 文本特征分析

一般将文本和图像数据称为非结构化数据，必须要经过一些特征提取和转换后才可以使用

- 将每个文本中的词都切开，叫做分词
- 每个文本都用一组词对应的数值来表示，就变成结构化数据

这种文本的特征表示方法称为词袋模型

### 词袋模型

#### 特点

- 由于 词典数量较多，单一文本中出现的词不会太多，所以用词袋模型表示的特征向量是非常**稀疏**的
- 忽略了文本的语法和不同词之间的顺序，仅仅将其看成是若干个词汇的集合，每个词的出现都是**独立**的
- 不同词语的重要性仅仅根据出现的**次数**来表示

#### 词权重表示方法

词在文本中出现的次数作为特征值，也称为词权重

- Bool表示：在文本中出现过记为1，没出现记为0
- 词频TF：词在文本中出现的次数
- TF-IDF

### TF-IDF

用词频TF来表示一个词在文本中的重要性会导致常用词的重要性被放大，但实际的重要性并没有那么大

TF-IDF就是用处理后的TF乘IDF来平衡词的重要性，TF一般做归一化处理，让一篇文档中所有词的TF值合计为1；IDF一般用对数变换，log(文档总数/(包含该词的文档数+1))

### 词向量模型

#### 词表征

词表征就是如何用向量的方式来表示一个词的特征，让计算机能够对词进行处理，常用的两种词表征的方式有

1. 词袋模型
    
    首先有一个词典，假设词典中有常用的10000个词，就可以用每一个词在词典中的位置来表征(编码)这个词，如果“我”这个词在词典中位于第2位，我们会用独热编码[0, 1, 0, ..., 0]来表示而不是直接用2。这个特征向量是高度稀疏的，所有词的词袋表示都是如此

2. 词向量模型

    英文叫做Word2Vec，又叫做词嵌入，可以解决词袋模型的稀疏性问题，核心思想是将每一个词映射到一个多维空间中，成为空间中的一个向量，一般这个空间的维数不会太高，在几百个，这几百维的特征向量是稠密的，向量中的每一个成员值都是非0的

#### 词向量的特性

词向量把每一个词映射到了一个高维空间中，用向量表示，向量的生成时基于词与词之间的相关性得来的，所以词向量有类比的特性，例如：vector("国王") - vector("王后") ≈ vector("男人") - vector("女人")

#### 词向量模型的基本原理

- 中心词：每一个待分析的词
- 邻居词：在文档语料中出现在中心词周围某个小窗口内的关联词
- 窗口大小：寻找邻居词的时候需要观察中心词的前后c个词

词向量模型的核心原理就是用邻居词的概率分布来作为中心的词向量表示

训练一个词向量模型有不同的方法，常用的有：
- 基于邻居词共现矩阵分解法
- 神经网络训练

#### 词向量到文档向量

词向量只是对词的特征表征，如果要对一篇文档进行特征表征可以用以下几种方法：
- 直接使用文档中所有词的词向量的平均值
- 使用文档中每个词的TF-IDF值作为权重，与每个词的词向量进行加权平均
- 根据文档中每个词的词向量对文档进行聚类，使用聚类后包含词最多的哪个类的中心点作为文档特征向量
- 使用doc2vec模型